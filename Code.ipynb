{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uv1rMnbSOfZb"
      },
      "source": [
        "# **Ανάλυση Ιατρικών Εικόνων MedMNIST με CNN, Transfer Learning & Vision Transformers (PyTorch)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Σκλαβενίτης Γεώργιος 10708 gsklaven@ece.auth.gr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKC0CxFJAUWH"
      },
      "source": [
        "# 0. Περιγραφή του Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CV8zo-m2PXkn",
        "outputId": "96b9a25c-8410-451c-db03-f237cd49af3f"
      },
      "outputs": [],
      "source": [
        "# Install the MedMNIST library\n",
        "!pip install medmnist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "20Fjvreom--m"
      },
      "outputs": [],
      "source": [
        "# All imports needed\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import torchvision.models as models\n",
        "from collections import Counter\n",
        "from torch import nn as nn\n",
        "from torch.utils.data import random_split\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "from sklearn.metrics import confusion_matrix, classification_report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RQHC2efDOq53",
        "outputId": "8dc23695-f59d-4315-fbba-c7247916acb9"
      },
      "outputs": [],
      "source": [
        "# Import dataset\n",
        "from medmnist import BloodMNIST\n",
        "\n",
        "# Create dataset splits (train, validation, test)\n",
        "# 'download=True' downloads the dataset if it doesn't exist\n",
        "# 'size=28' specifies the image resolution (28x28) as given in medmnist.com\n",
        "train_dataset = BloodMNIST(split='train', download=True, size=28)\n",
        "val_dataset = BloodMNIST(split='val', download=True, size=28)\n",
        "test_dataset = BloodMNIST(split='test', download=True, size=28)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jDBotQ6oTz8K",
        "outputId": "4ca66a9f-7b94-4347-d927-623f58e8715e"
      },
      "outputs": [],
      "source": [
        "# Check for other image dimensions (e.g., 32x32)\n",
        "try:\n",
        "    dataset_32 = BloodMNIST(split=\"train\", download=True, size=32)\n",
        "    img, _ = dataset_32[0]\n",
        "    print(f\"Test 32x32: Success. The images have size: {img.size}\")\n",
        "except Exception as e:\n",
        "    print(\"The 32x32 version is not available for this dataset.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wGf1D-kfQoPk",
        "outputId": "ae9d1792-f3bd-42da-888b-8d8ed899a8ed"
      },
      "outputs": [],
      "source": [
        "# Display basic image information\n",
        "first_image, first_label = train_dataset[0]\n",
        "image_array = np.array(first_image)\n",
        "\n",
        "print(f\"Image Information\")\n",
        "print(f\"Resolution: {image_array.shape}\")\n",
        "\n",
        "# Check if the image is RGB or Grayscale\n",
        "if len(image_array.shape) == 3 and image_array.shape[2] == 3:\n",
        "    print(\"Type: RGB\")\n",
        "else:\n",
        "    print(\"Type: Grayscale\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0q4momoiUpL8",
        "outputId": "a477f4e9-1cf8-4767-ca77-10ee234617a2"
      },
      "outputs": [],
      "source": [
        "# Display dataset and class information\n",
        "info = train_dataset.info\n",
        "\n",
        "description = info['description']\n",
        "print(f\"Description: {description}\")\n",
        "\n",
        "labels_map = info['label']\n",
        "\n",
        "print(f\"\\nClasses: {len(labels_map)}\")\n",
        "for key, value in labels_map.items():\n",
        "    print(f\"Label {key}: {value}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XyMPe1xzVLWq",
        "outputId": "1ca0cb6c-a3c7-4f05-b2e2-0517bfc2e5ea"
      },
      "outputs": [],
      "source": [
        "print(f\"Size of Splits\")\n",
        "print(f\"Training set: {len(train_dataset)} images\")\n",
        "print(f\"Validation set: {len(val_dataset)} images\")\n",
        "print(f\"Test set: {len(test_dataset)} images\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SBK4PmggbgcO",
        "outputId": "7a99efb5-93b8-465e-eb23-496f0d245369"
      },
      "outputs": [],
      "source": [
        "# Extract all labels from the training set to check distribution\n",
        "train_labels = [y[0] for _, y in train_dataset]\n",
        "counts = Counter(train_labels)\n",
        "\n",
        "print(f\"--- Class Distribution (Imbalance Check) ---\")\n",
        "print(\"Number of images per class in Training Set:\")\n",
        "for label_id, count in sorted(counts.items()):\n",
        "    class_name = labels_map[str(label_id)]\n",
        "    print(f\" - {class_name}: {count}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 247
        },
        "id": "wcJhnWAgbiuq",
        "outputId": "f6b1125f-0c3c-4512-9a76-906d3aa0046d"
      },
      "outputs": [],
      "source": [
        "# Visualize 5 random samples\n",
        "fig, axes = plt.subplots(1, 5, figsize=(15, 5))\n",
        "indices = np.random.choice(len(train_dataset), 5, replace=False)\n",
        "\n",
        "for i, idx in enumerate(indices):\n",
        "    img, label = train_dataset[idx]\n",
        "    class_name = labels_map[str(label[0])]\n",
        "\n",
        "    axes[i].imshow(img) # Display image\n",
        "    axes[i].set_title(class_name) # Set title\n",
        "    axes[i].axis('off') # Hide axes\n",
        "\n",
        "plt.suptitle(\"Sample Images from BloodMNIST\") # Central title\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZTHIKncM74Hq"
      },
      "source": [
        "# Global Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ieLpZKxzP9YV"
      },
      "outputs": [],
      "source": [
        "def train_loop(model, dataloader, optimizer, criterion, device):\n",
        "    \"\"\"\n",
        "    Executes one training cycle (epoch) for a model.\n",
        "\n",
        "    Args:\n",
        "        model (torch.nn.Module): The model to be trained.\n",
        "        dataloader (torch.utils.data.DataLoader): The DataLoader for the training set.\n",
        "        optimizer (torch.optim.Optimizer): The optimizer.\n",
        "        criterion (torch.nn.Module): The loss function.\n",
        "        device (torch.device): The device (CPU or GPU) on which training will be executed.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing the average loss and accuracy.\n",
        "    \"\"\"\n",
        "\n",
        "    # Set the model to training mode\n",
        "    model.train()\n",
        "\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    # Iterate over training data\n",
        "    for batch, (images, labels) in enumerate(dataloader):\n",
        "\n",
        "        # Transfer images and labels to the specified device\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        labels = labels.squeeze(1) # Convert labels to 1D (if necessary)\n",
        "\n",
        "        output = model(images) # Forward pass (input -> model -> prediction)\n",
        "        loss = criterion(output, labels) # Calculate the loss function\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        optimizer.zero_grad() # Zero out previous gradients\n",
        "        loss.backward() # Backpropagation (gradient calculation)\n",
        "        optimizer.step() # Update parameters\n",
        "\n",
        "        preds = output.argmax(dim=1) # Get the class with the highest probability\n",
        "        correct += (preds == labels).sum().item() # Count correct predictions\n",
        "        total += labels.size(0) # Count total samples\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "          current = batch * dataloader.batch_size + len(images)\n",
        "          print(f\"loss: {loss.item():>7f}  [{current:>5d}/{len(dataloader.dataset):>5d}]\")\n",
        "\n",
        "    return total_loss / len(dataloader), correct / total # Return average loss and accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S-7ZfVgIiV2w"
      },
      "outputs": [],
      "source": [
        "def test_loop(model, dataloader, criterion, device):\n",
        "    \"\"\"\n",
        "    Evaluates the performance of a model on a dataset.\n",
        "\n",
        "    Args:\n",
        "        model (torch.nn.Module): The model to be evaluated.\n",
        "        dataloader (torch.utils.data.DataLoader): The DataLoader for the validation/test set.\n",
        "        criterion (torch.nn.Module): The loss function.\n",
        "        device (torch.device): The device (CPU or GPU) on which the evaluation will be executed.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing the average loss, accuracy, all predictions\n",
        "               and all true labels.\n",
        "    \"\"\"\n",
        "\n",
        "    # Set the model to evaluation mode (disables dropout/batchnorm updates)\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    all_preds = [] # List to store all predictions\n",
        "    all_labels = [] # List to store all true labels\n",
        "\n",
        "    # Disable gradient calculation to save memory and speed\n",
        "    with torch.no_grad():\n",
        "        for images, labels in dataloader:\n",
        "\n",
        "            # Transfer images and labels to the specified device\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            labels = labels.squeeze(1) # Convert labels to 1D (if necessary)\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            preds = outputs.argmax(dim=1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "            all_preds.extend(preds.detach().cpu().numpy()) # Store predictions on CPU\n",
        "            all_labels.extend(labels.detach().cpu().numpy()) # Store labels on CPU\n",
        "\n",
        "    return total_loss / len(dataloader), correct / total, np.array(all_preds), np.array(all_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9AsdS_vfnJgj"
      },
      "outputs": [],
      "source": [
        "def run_training(model, train_loader, val_loader, optimizer, criterion, device, epochs):\n",
        "    \"\"\"\n",
        "    Executes the training and evaluation process of a model for a specified number of epochs.\n",
        "\n",
        "    Args:\n",
        "        model (torch.nn.Module): The model to be trained.\n",
        "        train_loader (torch.utils.data.DataLoader): DataLoader for the training set.\n",
        "        val_loader (torch.utils.data.DataLoader): DataLoader for the validation set.\n",
        "        optimizer (torch.optim.Optimizer): The optimizer.\n",
        "        criterion (torch.nn.Module): The loss function.\n",
        "        device (torch.device): The device (CPU or GPU).\n",
        "        epochs (int): The number of epochs.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary with the history of losses and accuracies for training and validation.\n",
        "    \"\"\"\n",
        "    history = {\n",
        "        'train_loss': [],\n",
        "        'train_acc': [],\n",
        "        'val_loss': [],\n",
        "        'val_acc': []\n",
        "    }\n",
        "\n",
        "    print(f\"Starting training on {device} for {epochs} epochs...\")\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Training\n",
        "        t_loss, t_acc = train_loop(model, train_loader, optimizer, criterion, device)\n",
        "\n",
        "        # Evaluation on the Validation set\n",
        "        v_loss, v_acc, _, _ = test_loop(model, val_loader, criterion, device)\n",
        "\n",
        "        # Store history\n",
        "        history['train_loss'].append(t_loss)\n",
        "        history['train_acc'].append(t_acc)\n",
        "        history['val_loss'].append(v_loss)\n",
        "        history['val_acc'].append(v_acc)\n",
        "\n",
        "        # Print progress per epoch\n",
        "        print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {t_loss:.4f} | Val Loss: {v_loss:.4f} | Val Acc: {v_acc:.4f}\")\n",
        "\n",
        "    return history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gDm3wZJNnZQP"
      },
      "outputs": [],
      "source": [
        "def evaluate_and_visualize(model, test_loader, criterion, device, history, class_names):\n",
        "    \"\"\"\n",
        "    Visualizes training curves, evaluates the model on the test set\n",
        "    and displays the confusion matrix and classification report.\n",
        "\n",
        "    Args:\n",
        "        model (torch.nn.Module): The trained model.\n",
        "        test_loader (torch.utils.data.DataLoader): DataLoader for the test set.\n",
        "        criterion (torch.nn.Module): The loss function.\n",
        "        device (torch.device): The device (CPU or GPU).\n",
        "        history (dict): The training history (losses, accuracies).\n",
        "        class_names (list): List of class names.\n",
        "    \"\"\"\n",
        "\n",
        "    # Visualize loss and accuracy curves\n",
        "    epochs_range = range(1, len(history['train_loss']) + 1)\n",
        "\n",
        "    plt.figure(figsize=(14, 5))\n",
        "\n",
        "    # Loss curve\n",
        "    plt.subplot(1, 2, 1) # Create a 1x2 subplot, in the 1st position\n",
        "    plt.plot(epochs_range, history['train_loss'], label='Training Loss')\n",
        "    plt.plot(epochs_range, history['val_loss'], label='Validation Loss', linestyle='--')\n",
        "    plt.title('Training & Validation Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    # Accuracy curve\n",
        "    plt.subplot(1, 2, 2) # Create a 1x2 subplot, in the 2nd position\n",
        "    plt.plot(epochs_range, history['val_acc'], label='Validation Accuracy', color='green')\n",
        "    plt.plot(epochs_range, history['train_acc'], label='Training Accuracy', color='blue')\n",
        "    plt.title('Training & Validation Accuracy')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    # Evaluate on Test Set\n",
        "    print(\"\\nEvaluating on Test Set...\")\n",
        "    test_loss, test_acc, preds, labels = test_loop(model, test_loader, criterion, device)\n",
        "    print(f\"Final Test Loss: {test_loss:.4f}\")\n",
        "    print(f\"Final Test Accuracy: {test_acc*100:.2f}%\")\n",
        "\n",
        "    # Confusion Matrix\n",
        "    cm = confusion_matrix(labels, preds)\n",
        "\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=class_names,\n",
        "                yticklabels=class_names)\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.show()\n",
        "\n",
        "    # Classification Report\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(labels, preds, target_names=class_names))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHzB06KWAcPW"
      },
      "source": [
        "# 1. CNN από την αρχή"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oYdfUhaAnZ6j"
      },
      "source": [
        "MODELS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y675B-eN84EU"
      },
      "outputs": [],
      "source": [
        "# Training Transformation\n",
        "# Introduce 'randomness' here to make the dataset more challenging and robust\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(p=0.5),  # 50% probability for horizontal flip\n",
        "    transforms.RandomRotation(degrees=10),   # Random rotation +/- 10 degrees\n",
        "    transforms.ToTensor()                    # Convert to Tensor (necessary for PyTorch)\n",
        "])\n",
        "\n",
        "# Validation & Test Transformation (WITHOUT Augmentation)\n",
        "# Keep the images 'clean' for accurate evaluation\n",
        "eval_transform = transforms.Compose([\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "# Define Datasets with appropriate transformations\n",
        "train_dataset = BloodMNIST(\n",
        "    split='train',\n",
        "    download=True,\n",
        "    size=28,\n",
        "    transform=train_transform # Apply training transformations\n",
        ")\n",
        "\n",
        "val_dataset = BloodMNIST(\n",
        "    split='val',\n",
        "    download=True,\n",
        "    size=28,\n",
        "    transform=eval_transform # Apply evaluation transformations\n",
        ")\n",
        "\n",
        "test_dataset = BloodMNIST(\n",
        "    split='test',\n",
        "    download=True,\n",
        "    size=28,\n",
        "    transform=eval_transform # Apply evaluation transformations\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F_qs3gp--OhB",
        "outputId": "f2160ffa-023e-4b28-c6c6-6f8e7df0fe38"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 64\n",
        "\n",
        "# Create DataLoaders for each split\n",
        "train_dataloader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True) # shuffle=True for random order in training data\n",
        "val_dataloader = DataLoader(dataset=val_dataset, batch_size=BATCH_SIZE, shuffle=False) # shuffle=False for consistent evaluation\n",
        "test_dataloader = DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "print(\"DataLoaders are ready!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-3MwVjRPe1oJ"
      },
      "source": [
        "SIMPLE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F5fQOkVS7wHM",
        "outputId": "64c71648-2b4a-47d9-9604-2fbe9e58b744"
      },
      "outputs": [],
      "source": [
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self, num_classes=8):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "\n",
        "        # --- BLOCK 1 ---\n",
        "        # Input image: [Batch, 3, 28, 28] (RGB)\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1) # 3 input channels (RGB)\n",
        "\n",
        "        # Output size calculation Conv1:\n",
        "        # Formula: ((input_size - kernel_size + 2 * padding) / stride) + 1\n",
        "        # (28 - 3 + 2 * 1) / 1 + 1 = 28\n",
        "        # Output shape after Conv1: [Batch, 32, 28, 28]\n",
        "\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # Output size calculation Pool1:\n",
        "        # 28 / 2 = 14\n",
        "        # Output shape after Pool1: [Batch, 32, 14, 14]\n",
        "\n",
        "\n",
        "        # --- BLOCK 2 ---\n",
        "        # Input to Conv2 is the output of Pool1: 14x14 with 32 channels\n",
        "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1) # 32 input channels, 64 output channels\n",
        "\n",
        "        # Output size calculation Conv2:\n",
        "        # (14 - 3 + 2 * 1) / 1 + 1 = 14\n",
        "        # Output shape after Conv2: [Batch, 64, 14, 14]\n",
        "\n",
        "        # Output size calculation Pool2:\n",
        "        # 14 / 2 = 7\n",
        "        # Output shape after Pool2: [Batch, 64, 7, 7]\n",
        "\n",
        "\n",
        "        # --- BLOCK 3 ---\n",
        "        # Input to Conv3 is the output of Pool2: 7x7 with 64 channels\n",
        "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
        "\n",
        "        # Output size calculation Conv3:\n",
        "        # (7 - 3 + 2 * 1) / 1 + 1 = 7\n",
        "        # Output shape after Conv3: [Batch, 128, 7, 7]\n",
        "\n",
        "        # Output size calculation Pool3:\n",
        "        # 7 / 2 = 3.5 -> PyTorch rounds down (floor) to an integer\n",
        "        # Result = 3\n",
        "        # Output shape after Pool3: [Batch, 128, 3, 3]\n",
        "\n",
        "\n",
        "        # Flatten and Fully Connected\n",
        "        # Final Tensor shape: [Batch, 128, 3, 3]\n",
        "        # Flattening: Channels * Height * Width\n",
        "        self.flatten_dim = 128 * 3 * 3  # = 1152\n",
        "        self.fc = nn.Linear(self.flatten_dim, num_classes) # Fully Connected layer with 8 output classes\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Apply Block 1\n",
        "        x = self.conv1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.pool(x)\n",
        "\n",
        "        # Apply Block 2\n",
        "        x = self.conv2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.pool(x)\n",
        "\n",
        "        # Apply Block 3\n",
        "        x = self.conv3(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.pool(x)\n",
        "\n",
        "        # Flatten parameters for the Linear layer\n",
        "        x = x.flatten(1)\n",
        "\n",
        "        # Classification\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "# Transfer the model to GPU if available, otherwise to CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = SimpleCNN(num_classes=8).to(device)\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "TZBwoPSPHhjO",
        "outputId": "6d096941-f546-4480-fc06-004902522127"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "\n",
        "# Seed for reproducibility of results\n",
        "torch.manual_seed(42)\n",
        "# Define epochs\n",
        "EPOCHS = 50\n",
        "# Define learning rate\n",
        "learning_rate = 1e-3\n",
        "\n",
        "# Initialize optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "# Define Loss Function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Run training\n",
        "history = run_training(model, train_dataloader, val_dataloader, optimizer, criterion, device, epochs=EPOCHS)\n",
        "\n",
        "# Retrieve class names for visualization\n",
        "class_names = [labels_map[str(i)] for i in range(8)]\n",
        "\n",
        "# Evaluate and visualize results\n",
        "evaluate_and_visualize(model, test_dataloader, criterion, device, history, class_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p0Mzc_BOe39i"
      },
      "source": [
        "BATCH NORMALIZATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z5OFQaW4RG5g",
        "outputId": "352cbd20-1c14-4e73-c2dc-81960fea51dd"
      },
      "outputs": [],
      "source": [
        "class CNNWithBatch(nn.Module):\n",
        "    def __init__(self, num_classes=8):\n",
        "        super(CNNWithBatch, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(32) # Batch Normalization after conv1\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(64) # Batch Normalization after conv2\n",
        "\n",
        "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(128) # Batch Normalization after conv3\n",
        "\n",
        "        self.flatten_dim = 128 * 3 * 3  # = 1152\n",
        "        self.fc = nn.Linear(self.flatten_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Apply Block 1\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x) # Apply Batch Normalization\n",
        "        x = F.relu(x)\n",
        "        x = self.pool(x)\n",
        "\n",
        "        # Apply Block 2\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.pool(x)\n",
        "\n",
        "        # Apply Block 3\n",
        "        x = self.conv3(x)\n",
        "        x = self.bn3(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.pool(x)\n",
        "\n",
        "        # Flatten parameters for the Linear layer\n",
        "        x = x.flatten(1)\n",
        "\n",
        "        # Classification\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "# Transfer the model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model_with_batch = CNNWithBatch(num_classes=8).to(device)\n",
        "print(model_with_batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "_IemZVX4Ri1D",
        "outputId": "fdb66876-0156-4915-a0e7-19bfa2a00a3a"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "\n",
        "# Seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "# Define epochs\n",
        "EPOCHS = 45\n",
        "# Define learning rate\n",
        "learning_rate = 5e-4\n",
        "\n",
        "# Initialize optimizer\n",
        "optimizer = torch.optim.Adam(model_with_batch.parameters(), lr=learning_rate)\n",
        "# Define Loss Function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Run training\n",
        "history = run_training(model_with_batch, train_dataloader, val_dataloader, optimizer, criterion, device, epochs=EPOCHS)\n",
        "# Retrieve class names for visualization\n",
        "class_names = [labels_map[str(i)] for i in range(8)]\n",
        "# Evaluate and visualize results\n",
        "evaluate_and_visualize(model_with_batch, test_dataloader, criterion, device, history, class_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1zThPEZzPavG"
      },
      "source": [
        "LAYER NORMALIZATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AuLkgKFzPaKr"
      },
      "outputs": [],
      "source": [
        "class CNNWithLayerNorm(nn.Module):\n",
        "    def __init__(self, num_classes=8):\n",
        "        super(CNNWithLayerNorm, self).__init__()\n",
        "\n",
        "        # --- BLOCK 1 ---\n",
        "        # Input: [Batch, 3, 28, 28]\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # After conv1 + pool, spatial dimensions are 14x14 with 32 channels.\n",
        "        # LayerNorm requires the exact shape of the feature map (C, H, W).\n",
        "        self.ln1 = nn.LayerNorm([32, 14, 14])\n",
        "\n",
        "        # --- BLOCK 2 ---\n",
        "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
        "\n",
        "        # After conv2 + pool, spatial dimensions are 7x7 with 64 channels.\n",
        "        self.ln2 = nn.LayerNorm([64, 7, 7])\n",
        "\n",
        "        # --- BLOCK 3 ---\n",
        "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
        "\n",
        "        # After conv3 + pool, spatial dimensions are 3x3 with 128 channels.\n",
        "        self.ln3 = nn.LayerNorm([128, 3, 3])\n",
        "\n",
        "        # --- FLATTEN & FULLY CONNECTED ---\n",
        "        self.flatten_dim = 128 * 3 * 3\n",
        "        self.fc = nn.Linear(self.flatten_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Block 1\n",
        "        x = self.conv1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.pool(x)\n",
        "        x = self.ln1(x)  # Apply Layer Norm at the end of the block\n",
        "\n",
        "        # Block 2\n",
        "        x = self.conv2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.pool(x)\n",
        "        x = self.ln2(x)\n",
        "\n",
        "        # Block 3\n",
        "        x = self.conv3(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.pool(x)\n",
        "        x = self.ln3(x)\n",
        "\n",
        "        # Flatten and classify\n",
        "        x = x.flatten(1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "# Transfer the model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# Initialize model\n",
        "model_ln = CNNWithLayerNorm(num_classes=8).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "EJvCIVkvPfR7",
        "outputId": "9946289a-e103-4478-92d9-c7bb917fe2d4"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "\n",
        "# Seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "# Define epochs\n",
        "EPOCHS = 45\n",
        "# Define learning rate\n",
        "learning_rate = 5e-4\n",
        "\n",
        "# Initialize optimizer\n",
        "optimizer = torch.optim.Adam(model_ln.parameters(), lr=learning_rate)\n",
        "# Define Loss Function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Run training\n",
        "history_ln = run_training(model_ln, train_dataloader, val_dataloader, optimizer, criterion, device, epochs=EPOCHS)\n",
        "class_names = [labels_map[str(i)] for i in range(8)]\n",
        "# Evaluate\n",
        "evaluate_and_visualize(model_ln, test_dataloader, criterion, device, history_ln, class_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_5tBb1Oii3R_"
      },
      "source": [
        "DROPOUT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0roiZ2ZyR0Cj"
      },
      "outputs": [],
      "source": [
        "class CNNWithDropout(nn.Module):\n",
        "    def __init__(self, num_classes=8, drop_percent=0.5):\n",
        "        super(CNNWithDropout, self).__init__()\n",
        "\n",
        "        # Input: 28x28 -> Output: 14x14\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "\n",
        "        # Input: 14x14 -> Output: 7x7\n",
        "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "\n",
        "        # Input: 7x7 -> Output: 3x3\n",
        "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(128)\n",
        "\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # Dropout layer\n",
        "        self.dropout = nn.Dropout(p=drop_percent)\n",
        "\n",
        "        # Flatten\n",
        "        self.flatten_dim = 128 * 3 * 3\n",
        "        self.fc = nn.Linear(self.flatten_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Block 1\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.pool(x)\n",
        "\n",
        "        # Block 2\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.pool(x)\n",
        "\n",
        "        # Block 3\n",
        "        x = self.conv3(x)\n",
        "        x = self.bn3(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.pool(x)\n",
        "\n",
        "        # Flatten\n",
        "        x = x.flatten(1)\n",
        "\n",
        "        # Apply Dropout before the final Fully Connected layer\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        # Classification\n",
        "        x = self.fc(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "95XFJxLqSSQQ",
        "outputId": "e636a751-4a1c-4d53-eea5-86c0f1c5c94a"
      },
      "outputs": [],
      "source": [
        "# Transfer the model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Hyperparameters\n",
        "# Seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "# Define epochs\n",
        "EPOCHS = 35\n",
        "# Define learning rate\n",
        "learning_rate = 1e-3\n",
        "\n",
        "# Define list of dropout rates to test\n",
        "dropout_rates = [0.2, 0.5, 0.7]\n",
        "\n",
        "# Iterate for each dropout rate\n",
        "for dp in dropout_rates:\n",
        "    print(f\"Training with Dropout: {dp}\")\n",
        "\n",
        "    # Initialize a new model for each dropout rate\n",
        "    model_with_dropout = CNNWithDropout(num_classes=8, drop_percent=dp).to(device)\n",
        "\n",
        "    # Initialize optimizer\n",
        "    optimizer = torch.optim.Adam(model_with_dropout.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Define Loss Function\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    history = run_training(model_with_dropout, train_dataloader, val_dataloader, optimizer, criterion, device, epochs=EPOCHS)\n",
        "    class_names = [labels_map[str(i)] for i in range(8)]\n",
        "    evaluate_and_visualize(model_with_dropout, test_dataloader, criterion, device, history, class_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOqA17JCi1jn"
      },
      "source": [
        "WEIGHT DECAY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "8wKjYHsOT9Np",
        "outputId": "54e28a65-212d-4bde-b21b-e757d7585d67"
      },
      "outputs": [],
      "source": [
        "# Transfer the model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Hyperparameters\n",
        "# Seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "# Define epochs\n",
        "EPOCHS = 35\n",
        "# Define learning rate\n",
        "learning_rate = 1e-4\n",
        "\n",
        "# Weight Decay values to test\n",
        "weight_decay_values = [1e-4, 1e-3, 1e-2]\n",
        "\n",
        "# Iterate for each Weight Decay value\n",
        "for wd in weight_decay_values:\n",
        "    print(f\"Training with Weight Decay: {wd}\")\n",
        "\n",
        "    # Initialize model with Dropout Percent = 0.2 (for consistency)\n",
        "    model_with_weight_decay = CNNWithDropout(num_classes=8, drop_percent=0.2).to(device)\n",
        "\n",
        "    # Initialize optimizer for this specific Weight Decay value\n",
        "    optimizer = torch.optim.Adam(\n",
        "        model_with_weight_decay.parameters(),\n",
        "        lr=learning_rate,\n",
        "        weight_decay=wd # Apply Weight Decay in the optimizer\n",
        "    )\n",
        "\n",
        "    # Define Loss Function\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    history = run_training(model_with_weight_decay, train_dataloader, val_dataloader, optimizer, criterion, device, epochs=EPOCHS)\n",
        "    class_names = [labels_map[str(i)] for i in range(8)]\n",
        "    evaluate_and_visualize(model_with_weight_decay, test_dataloader, criterion, device, history, class_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uvDNOarqe3L3"
      },
      "source": [
        "MIX IT UP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "FHb0OjxQe2Eq",
        "outputId": "b0886a2a-ee6e-41d4-94a9-8ed2773264fd"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# 1. Define the Class with Layer Normalization & Dropout\n",
        "class CNNLayerNormDropout(nn.Module):\n",
        "    def __init__(self, num_classes=8, drop_percent=0.2):\n",
        "        super(CNNLayerNormDropout, self).__init__()\n",
        "\n",
        "        # Block 1: Conv -> ReLU -> Pool -> LayerNorm\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
        "        self.pool1 = nn.MaxPool2d(2, 2)\n",
        "        self.ln1 = nn.LayerNorm([32, 14, 14])\n",
        "\n",
        "        # Block 2: Conv -> ReLU -> Pool -> LayerNorm\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.pool2 = nn.MaxPool2d(2, 2)\n",
        "        self.ln2 = nn.LayerNorm([64, 7, 7])\n",
        "\n",
        "        # Block 3: Conv -> ReLU -> Pool -> LayerNorm\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.pool3 = nn.MaxPool2d(2, 2)\n",
        "        self.ln3 = nn.LayerNorm([128, 3, 3])\n",
        "\n",
        "        # Classifier: Flatten -> Dropout -> Linear\n",
        "        self.flatten_dim = 128 * 3 * 3\n",
        "        self.fc = nn.Linear(self.flatten_dim, num_classes)\n",
        "        self.dropout = nn.Dropout(p=drop_percent) # Dropout before the FC layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Approach: Conv  -> ReLU -> Pool -> LayerNorm\n",
        "        x = self.conv1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.pool1(x)\n",
        "        x = self.ln1(x)\n",
        "\n",
        "        x = self.conv2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.pool2(x)\n",
        "        x = self.ln2(x)\n",
        "\n",
        "        x = self.conv3(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.pool3(x)\n",
        "        x = self.ln3(x)\n",
        "\n",
        "        x = x.flatten(1)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "torch.manual_seed(42) # Seed for reproducibility\n",
        "\n",
        "# Fixed parameters for the \"Ultimate Combo\"\n",
        "EPOCHS = 40\n",
        "BEST_WD = 1e-4 # Optimal Weight Decay value from previous experiments\n",
        "BEST_DROP = 0.2 # Optimal Dropout value from previous experiments\n",
        "\n",
        "# Learning Rate values to test\n",
        "learning_rate_values = [1e-4, 1e-3, 5e-3]\n",
        "\n",
        "# --- Experiment Loop ---\n",
        "for lr in learning_rate_values:\n",
        "    print(f\" Training with Learning Rate: {lr} \")\n",
        "    print(f\" (Config: LayerNorm, Drop={BEST_DROP}, WD={BEST_WD})\")\n",
        "\n",
        "    # Initialize NEW model in each loop\n",
        "    model = CNNLayerNormDropout(num_classes=8, drop_percent=BEST_DROP).to(device)\n",
        "\n",
        "    # Optimizer with current LR and fixed Weight Decay\n",
        "    optimizer = torch.optim.Adam(\n",
        "        model.parameters(),\n",
        "        lr=lr,\n",
        "        weight_decay=BEST_WD\n",
        "    )\n",
        "\n",
        "    # Loss Function\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Training\n",
        "    history = run_training(model, train_dataloader, val_dataloader, optimizer, criterion, device, epochs=EPOCHS)\n",
        "\n",
        "    # Evaluation\n",
        "    print(f\"Evaluating LR: {lr}...\")\n",
        "    class_names = [labels_map[str(i)] for i in range(8)]\n",
        "    evaluate_and_visualize(model, test_dataloader, criterion, device, history, class_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g4uK8gc6Am-o"
      },
      "source": [
        "# 2. Transfer Learning με CNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kdEeUWkvAt3g"
      },
      "source": [
        "Feature Extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l9wEv-n8ORLm",
        "outputId": "addcf698-3524-41cb-a873-55481f1022f0"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE_FE = 64\n",
        "\n",
        "train_dataloader_fe = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE_FE, shuffle=True)\n",
        "val_dataloader_fe = DataLoader(dataset=val_dataset, batch_size=BATCH_SIZE_FE, shuffle=False)\n",
        "test_dataloader_fe = DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE_FE, shuffle=False)\n",
        "\n",
        "print(\"DataLoaders are ready!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "alX50c3iAuNT",
        "outputId": "f8a090f8-1f8a-4ce5-9f27-cc816e554f95"
      },
      "outputs": [],
      "source": [
        "# Load pre-trained ResNet18\n",
        "feature_model = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
        "\n",
        "# \"Freeze\" all parameters of the base model\n",
        "for param in feature_model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Replace the classifier head\n",
        "num_features = feature_model.fc.in_features # Number of incoming features to the last FC layer\n",
        "feature_model.fc = nn.Linear(num_features, 8) # New FC layer with 8 outputs\n",
        "\n",
        "# \"Unfreeze\" the parameters of the new classifier head\n",
        "for param in feature_model.fc.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "# Transfer the model to the device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "feature_model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "thr6sFlHEROw",
        "outputId": "d26f9ffd-81f4-46b3-b2e5-abc14bd51d14"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "# Seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "# Define epochs\n",
        "EPOCHS = 20\n",
        "# Define learning rate\n",
        "learning_rate = 1e-3\n",
        "\n",
        "# We choose optimizer only for the trainable (fc) layers.\n",
        "optimizer = torch.optim.Adam(feature_model.fc.parameters(), lr=learning_rate)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Training (Feature Extraction)\n",
        "print(f\"Starting Feature Extraction\")\n",
        "history_fe = run_training(feature_model, train_dataloader_fe, val_dataloader_fe, optimizer, criterion, device, EPOCHS)\n",
        "\n",
        "class_names = [labels_map[str(i)] for i in range(8)]\n",
        "evaluate_and_visualize(feature_model, test_dataloader_fe, criterion, device, history_fe, class_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fgDgyIf0Jkq3"
      },
      "source": [
        "Fine Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZiVnyWtMLDaq",
        "outputId": "9ff0eb6f-a5b0-48ba-b226-a2903c48505a"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE_FT = 32 # Smaller batch size for fine-tuning\n",
        "\n",
        "train_dataloader_fine = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE_FT, shuffle=True)\n",
        "val_dataloader_fine = DataLoader(dataset=val_dataset, batch_size=BATCH_SIZE_FT, shuffle=False)\n",
        "test_dataloader_fine = DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE_FT, shuffle=False)\n",
        "\n",
        "print(\"DataLoaders are ready!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OeEUW-D5Jt7s",
        "outputId": "bbd3a540-c2f3-4758-f9f5-f03241e6562a"
      },
      "outputs": [],
      "source": [
        "transfer_model = feature_model # Continue with the model trained in Feature Extraction\n",
        "\n",
        "# \"Freeze\" all parameters initially\n",
        "for param in transfer_model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# \"Unfreeze\" the last convolutional blocks (e.g., layer3 and layer4 of ResNet)\n",
        "for param in transfer_model.layer3.parameters():\n",
        "    param.requires_grad = True\n",
        "for param in transfer_model.layer4.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "# Also, the classifier head must be unfrozen\n",
        "for param in transfer_model.fc.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "transfer_model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "FwEiZWmcKeLT",
        "outputId": "d433df47-a581-4ccc-9a5e-0fa93fbc4d9b"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "# Seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "# Define epochs\n",
        "EPOCHS = 15 # Fewer epochs for fine-tuning, as the model is already close to a solution\n",
        "# Define learning rate\n",
        "learning_rate = 1e-4\n",
        "\n",
        "# We choose optimizer only for the parameters that have been set as trainable (requires_grad=True)\n",
        "optimizer_fine = torch.optim.Adam(filter(lambda p: p.requires_grad, transfer_model.parameters()), lr=learning_rate)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "print(f\"Starting Fine Tuning\")\n",
        "history_tr = run_training(transfer_model, train_dataloader_fine, val_dataloader_fine, optimizer_fine, criterion, device, EPOCHS)\n",
        "\n",
        "class_names = [labels_map[str(i)] for i in range(8)]\n",
        "evaluate_and_visualize(transfer_model, test_dataloader_fine, criterion, device, history_tr, class_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tE4U-CPLRAyp"
      },
      "source": [
        "RESIZE BEFORE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "81xUjGiARAZY"
      },
      "outputs": [],
      "source": [
        "# 1. Training Transformation (with Data Augmentation)\n",
        "# Includes Resize to 224x224 and Normalization specifically for ImageNet pre-trained models\n",
        "train_transform_new = transforms.Compose([\n",
        "    transforms.Resize(224), # Resize to 224x224 pixels\n",
        "    transforms.RandomHorizontalFlip(p=0.5),  # 50% probability for horizontal flip\n",
        "    transforms.RandomRotation(degrees=10),   # Random rotation +/- 10 degrees\n",
        "    transforms.ToTensor(),                    # Convert to Tensor\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], # ImageNet mean and standard deviation\n",
        "                         std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "# 2. Validation & Test Transformation (WITHOUT Augmentation)\n",
        "# Includes Resize to 224x224 and Normalization\n",
        "eval_transform_new = transforms.Compose([\n",
        "    transforms.Resize(224), # Resize to 224x224 pixels\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], # ImageNet mean and standard deviation\n",
        "                         std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "# 3. Define Datasets with appropriate transformations\n",
        "train_dataset = BloodMNIST(\n",
        "    split='train',\n",
        "    download=True,\n",
        "    size=28, # The original dataset is 28x28, but it will be resized with the transform\n",
        "    transform=train_transform_new\n",
        ")\n",
        "\n",
        "val_dataset = BloodMNIST(\n",
        "    split='val',\n",
        "    download=True,\n",
        "    size=28,\n",
        "    transform=eval_transform_new\n",
        ")\n",
        "\n",
        "test_dataset = BloodMNIST(\n",
        "    split='test',\n",
        "    download=True,\n",
        "    size=28,\n",
        "    transform=eval_transform_new\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CZ4qrxs7RbsV",
        "outputId": "4c5473c2-bbd8-4503-d662-f75bd300a763"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE_FE = 64\n",
        "\n",
        "train_dataloader_fe2 = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE_FE, shuffle=True)\n",
        "val_dataloader_fe2 = DataLoader(dataset=val_dataset, batch_size=BATCH_SIZE_FE, shuffle=False)\n",
        "test_dataloader_fe2 = DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE_FE, shuffle=False)\n",
        "\n",
        "print(\"DataLoaders are ready!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s4lEg0TlRfYP",
        "outputId": "7493660f-b54f-420f-ebe0-9b87fbd49829"
      },
      "outputs": [],
      "source": [
        "# Load pre-trained ResNet18\n",
        "feature_model = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
        "\n",
        "# \"Freeze\" all parameters of the base model\n",
        "for param in feature_model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Replace the classifier head for our problem (8 classes)\n",
        "num_features = feature_model.fc.in_features\n",
        "feature_model.fc = nn.Linear(num_features, 8)\n",
        "\n",
        "# \"Unfreeze\" the parameters of the new classifier head\n",
        "for param in feature_model.fc.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "feature_model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "JnUR33jtRh5q",
        "outputId": "d8597fa0-0e68-46cd-b73d-a845c4705259"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "# Seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "# Define epochs\n",
        "EPOCHS = 20\n",
        "# Define learning rate\n",
        "learning_rate = 1e-3\n",
        "\n",
        "# We choose optimizer only for the trainable (fc) layers.\n",
        "optimizer = torch.optim.Adam(feature_model.fc.parameters(), lr=learning_rate)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Training (Feature Extraction)\n",
        "print(f\"Starting Feature Extraction\")\n",
        "history_fe = run_training(feature_model, train_dataloader_fe2, val_dataloader_fe2, optimizer, criterion, device, EPOCHS)\n",
        "\n",
        "class_names = [labels_map[str(i)] for i in range(8)]\n",
        "evaluate_and_visualize(feature_model, test_dataloader_fe2, criterion, device, history_fe, class_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fTBLuXVyRic3",
        "outputId": "fa6f4af0-9017-4b96-d41d-9d9540149e68"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE_FT = 32\n",
        "\n",
        "train_dataloader_fine2 = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE_FT, shuffle=True)\n",
        "val_dataloader_fine2 = DataLoader(dataset=val_dataset, batch_size=BATCH_SIZE_FT, shuffle=False)\n",
        "test_dataloader_fine2 = DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE_FT, shuffle=False)\n",
        "\n",
        "print(\"DataLoaders are ready!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g7tfaskNRmFD",
        "outputId": "e26851c2-20ab-4b4b-b3b1-5dde6d709a20"
      },
      "outputs": [],
      "source": [
        "transfer_model = feature_model # Continue with the model trained in Feature Extraction\n",
        "\n",
        "# \"Freeze\" all parameters initially\n",
        "for param in transfer_model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# \"Unfreeze\" the last convolutional blocks (e.g., layer3 and layer4 of ResNet)\n",
        "for param in transfer_model.layer3.parameters():\n",
        "    param.requires_grad = True\n",
        "for param in transfer_model.layer4.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "# Also, the classifier head must be unfrozen\n",
        "for param in transfer_model.fc.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "transfer_model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "aR06Eb1pRqOW",
        "outputId": "5f4b3bc6-26c7-4ad8-d0fa-44a38eae93da"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "# Seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "# Define epochs\n",
        "EPOCHS = 15\n",
        "# Define learning rate\n",
        "learning_rate = 1e-4\n",
        "\n",
        "# We choose optimizer only for the parameters that have been set as trainable (requires_grad=True)\n",
        "optimizer_fine = torch.optim.Adam(filter(lambda p: p.requires_grad, transfer_model.parameters()), lr=learning_rate)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "print(f\"Starting Fine Tuning\")\n",
        "history_tr = run_training(transfer_model, train_dataloader_fine2, val_dataloader_fine2, optimizer_fine, criterion, device, EPOCHS)\n",
        "\n",
        "class_names = [labels_map[str(i)] for i in range(8)]\n",
        "evaluate_and_visualize(transfer_model, test_dataloader_fine2, criterion, device, history_tr, class_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O78b4mcLAqgw"
      },
      "source": [
        "# 3. Μικρός Vision Transformer (DeiT)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CCRDg1Pi7y9Z"
      },
      "outputs": [],
      "source": [
        "# Independent run for easy use\n",
        "import timm # Import the timm library for Vision Transformers\n",
        "from medmnist import BloodMNIST\n",
        "\n",
        "# Create dataset splits (train, validation, test)\n",
        "# 'size=28' specifies the image resolution (28x28)\n",
        "train_dataset = BloodMNIST(split='train', download=True, size=28)\n",
        "val_dataset = BloodMNIST(split='val', download=True, size=28)\n",
        "test_dataset = BloodMNIST(split='test', download=True, size=28)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iTb5P-Ps8QvU"
      },
      "outputs": [],
      "source": [
        "# 1. Training Transformation (with Data Augmentation)\n",
        "# Includes Resize to 224x224 (as required by DeiT) and Normalization\n",
        "train_transform_new = transforms.Compose([\n",
        "    transforms.Resize(224), # Resize to 224x224 pixels, necessary for DeiT\n",
        "    transforms.RandomHorizontalFlip(p=0.5),  # 50% probability for horizontal flip\n",
        "    transforms.RandomRotation(degrees=10),   # Random rotation +/- 10 degrees\n",
        "    transforms.ToTensor(),                    # Convert to Tensor\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], # ImageNet mean and standard deviation\n",
        "                         std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "# 2. Validation & Test Transformation (WITHOUT Augmentation)\n",
        "# Includes Resize to 224x224 and Normalization\n",
        "eval_transform_new = transforms.Compose([\n",
        "    transforms.Resize(224), # Resize to 224x224 pixels\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], # ImageNet mean and standard deviation\n",
        "                         std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "# 3. Define Datasets with appropriate transformations\n",
        "train_dataset = BloodMNIST(\n",
        "    split='train',\n",
        "    download=True,\n",
        "    size=28,\n",
        "    transform=train_transform_new\n",
        ")\n",
        "\n",
        "val_dataset = BloodMNIST(\n",
        "    split='val',\n",
        "    download=True,\n",
        "    size=28,\n",
        "    transform=eval_transform_new\n",
        ")\n",
        "\n",
        "test_dataset = BloodMNIST(\n",
        "    split='test',\n",
        "    download=True,\n",
        "    size=28,\n",
        "    transform=eval_transform_new\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JrvBhvQm9IbZ",
        "outputId": "7f6c679d-e522-4aa0-9e20-0cb123543c0f"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE_TIM = 32 # Smaller batch size for ViT\n",
        "\n",
        "train_dataloader_tim = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE_TIM, shuffle=True)\n",
        "val_dataloader_tim = DataLoader(dataset=val_dataset, batch_size=BATCH_SIZE_TIM, shuffle=False)\n",
        "test_dataloader_tim = DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE_TIM, shuffle=False)\n",
        "\n",
        "print(\"DataLoaders are ready!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "d52628ad5cbe4f8a834ac62c26ee8ed2",
            "d2d97fff1a2e4bd098cc3e9f4e6427a3",
            "7e27f3ffa648400b8b2981b194b9afd8",
            "50a806e194304b859faaa1d791727feb",
            "ef96e1d96f9b4c058afb973fb7442042",
            "f972a6c30508435eafa9470d7480eb23",
            "d0cfcad1c98e4c59b73d7be480c39a8f",
            "282c616437074652b714b80708a25622",
            "eed9da114c1946dcbe49eb7ebe7d6efb",
            "64a0ee45f9d54209b49a32fa974b098a",
            "5f8a423e54094bc19a6dba558bfde161"
          ]
        },
        "id": "OE-RjJwm9620",
        "outputId": "43266df8-ba11-4a3b-b007-b4a3da3aee9d"
      },
      "outputs": [],
      "source": [
        "# Load pre-trained DeiT (Tiny version)\n",
        "vit_model = timm.create_model('deit_tiny_patch16_224', pretrained=True) # pretrained=True loads weights from ImageNet\n",
        "\n",
        "# \"Freeze\" all parameters of the base model\n",
        "for param in vit_model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Adapt Classifier Head\n",
        "num_in_features = vit_model.head.in_features # Number of incoming features to the head\n",
        "vit_model.head = nn.Linear(num_in_features, 8) # 8 classes for BloodMNIST\n",
        "\n",
        "# Transfer the model to the device\n",
        "vit_model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Al7ta8qf9OWK",
        "outputId": "f62073cd-a340-4e37-8a2b-ebde04d81779"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "EPOCHS_FE = 15 # Fewer epochs for feature extraction\n",
        "LR_FE = 1e-3\n",
        "\n",
        "# Optimizer trains only the classifier head\n",
        "optimizer_fe = torch.optim.Adam(vit_model.head.parameters(), lr=LR_FE)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "print(f\"Starting Feature Extraction (Only Head) for {EPOCHS_FE} epochs...\")\n",
        "history_vit_fe = run_training(vit_model, train_dataloader_tim, val_dataloader_tim, optimizer_fe, criterion, device, EPOCHS_FE)\n",
        "\n",
        "class_names = [labels_map[str(i)] for i in range(8)]\n",
        "evaluate_and_visualize(vit_model, test_dataloader_tim, criterion, device, history_vit_fe, class_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "lggcLgpy8_kg",
        "outputId": "1749cf03-25c5-4a0a-af53-9645fd982c06"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE_FT = 16\n",
        "train_loader_ft = DataLoader(train_dataset, batch_size=BATCH_SIZE_FT, shuffle=True)\n",
        "val_loader_ft = DataLoader(val_dataset, batch_size=BATCH_SIZE_FT, shuffle=False)\n",
        "test_loader_ft = DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE_FT, shuffle=False)\n",
        "\n",
        "# Unfreeze the last Blocks\n",
        "for param in vit_model.blocks[-2:].parameters(): # Unfreeze the last 2 transformer blocks\n",
        "    param.requires_grad = True\n",
        "\n",
        "# Also, the norm and head must be unfrozen\n",
        "for param in vit_model.norm.parameters():\n",
        "    param.requires_grad = True\n",
        "for param in vit_model.head.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "# Fine-Tuning Hyperparamaters\n",
        "EPOCHS_FT = 10\n",
        "LR_FT = 1e-4\n",
        "\n",
        "# Optimizer for the unfrozen parameters\n",
        "optimizer_ft = torch.optim.Adam(filter(lambda p: p.requires_grad, vit_model.parameters()), lr=LR_FT)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "history_vit_ft = run_training(vit_model, train_loader_ft, val_loader_ft, optimizer_ft, criterion, device, EPOCHS_FT)\n",
        "\n",
        "# Evaluation\n",
        "class_names = [labels_map[str(i)] for i in range(8)]\n",
        "evaluate_and_visualize(vit_model, test_loader_ft, criterion, device, history_vit_ft, class_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HIqfmutjj15i"
      },
      "source": [
        "# Βελτιώσεις"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "mPlYxtEBj3rS",
        "outputId": "e5df4000-4e7f-4c07-8f62-b1d2b88c0967"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Model Definition (CNN with Dropout & Batch Norm)\n",
        "class CNNWithDropout(nn.Module):\n",
        "    def __init__(self, num_classes=8, drop_percent=0.5):\n",
        "        super(CNNWithDropout, self).__init__()\n",
        "        # Input: 28x28 -> Output: 14x14\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "\n",
        "        # Input: 14x14 -> Output: 7x7\n",
        "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "\n",
        "        # Input: 7x7 -> Output: 3x3\n",
        "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(128)\n",
        "\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.dropout = nn.Dropout(p=drop_percent)\n",
        "        self.flatten_dim = 128 * 3 * 3\n",
        "        self.fc = nn.Linear(self.flatten_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
        "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
        "        x = self.pool(F.relu(self.bn3(self.conv3(x))))\n",
        "        x = x.flatten(1)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "torch.manual_seed(42)\n",
        "\n",
        "EPOCHS = 40\n",
        "LR = 1e-3\n",
        "WEIGHT_DECAY = 1e-4\n",
        "DROP_PERCENT = 0.2  # Optimal values\n",
        "\n",
        "# Model Initialization\n",
        "model = CNNWithDropout(num_classes=8, drop_percent=DROP_PERCENT).to(device)\n",
        "\n",
        "# Calculate weights for Class Imbalance\n",
        "class_counts = torch.tensor([852, 2181, 1085, 2026, 849, 993, 2330, 1643], dtype=torch.float)\n",
        "weights = 1.0 / class_counts      # Inverse frequency\n",
        "weights = weights / weights.sum() # Normalize\n",
        "weights = weights.to(device)\n",
        "\n",
        "# Define Loss with Label Smoothing\n",
        "criterion = nn.CrossEntropyLoss(weight=weights, label_smoothing=0.1)\n",
        "\n",
        "# Optimizer with Weight Decay\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
        "\n",
        "# Learning Rate Scheduler (Cosine Annealing)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
        "\n",
        "print(\"Starting Advanced Experiment: Weighted Loss + Scheduler + WD\")\n",
        "\n",
        "history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    # Training Step\n",
        "    train_loss, train_acc = train_loop(model, train_dataloader, optimizer, criterion, device)\n",
        "\n",
        "    # Validation Step\n",
        "    val_loss, val_acc, _, _ = test_loop(model, val_dataloader, criterion, device)\n",
        "\n",
        "    # Scheduler Step (Update LR after validation)\n",
        "    scheduler.step()\n",
        "\n",
        "    # Logging\n",
        "    history['train_loss'].append(train_loss)\n",
        "    history['train_acc'].append(train_acc)\n",
        "    history['val_loss'].append(val_loss)\n",
        "    history['val_acc'].append(val_acc)\n",
        "\n",
        "    current_lr = optimizer.param_groups[0]['lr']\n",
        "    print(f\"Epoch {epoch+1}/{EPOCHS} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f} | LR: {current_lr:.6f}\")\n",
        "\n",
        "# Final Evaluation\n",
        "print(\"Evaluating on Test Set...\")\n",
        "class_names = [labels_map[str(i)] for i in range(8)]\n",
        "evaluate_and_visualize(model, test_dataloader, criterion, device, history, class_names)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "g4uK8gc6Am-o",
        "O78b4mcLAqgw"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "282c616437074652b714b80708a25622": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "50a806e194304b859faaa1d791727feb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_64a0ee45f9d54209b49a32fa974b098a",
            "placeholder": "​",
            "style": "IPY_MODEL_5f8a423e54094bc19a6dba558bfde161",
            "value": " 22.9M/22.9M [00:01&lt;00:00, 18.7MB/s]"
          }
        },
        "5f8a423e54094bc19a6dba558bfde161": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "64a0ee45f9d54209b49a32fa974b098a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7e27f3ffa648400b8b2981b194b9afd8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_282c616437074652b714b80708a25622",
            "max": 22883348,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_eed9da114c1946dcbe49eb7ebe7d6efb",
            "value": 22883348
          }
        },
        "d0cfcad1c98e4c59b73d7be480c39a8f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d2d97fff1a2e4bd098cc3e9f4e6427a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f972a6c30508435eafa9470d7480eb23",
            "placeholder": "​",
            "style": "IPY_MODEL_d0cfcad1c98e4c59b73d7be480c39a8f",
            "value": "model.safetensors: 100%"
          }
        },
        "d52628ad5cbe4f8a834ac62c26ee8ed2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d2d97fff1a2e4bd098cc3e9f4e6427a3",
              "IPY_MODEL_7e27f3ffa648400b8b2981b194b9afd8",
              "IPY_MODEL_50a806e194304b859faaa1d791727feb"
            ],
            "layout": "IPY_MODEL_ef96e1d96f9b4c058afb973fb7442042"
          }
        },
        "eed9da114c1946dcbe49eb7ebe7d6efb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ef96e1d96f9b4c058afb973fb7442042": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f972a6c30508435eafa9470d7480eb23": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
